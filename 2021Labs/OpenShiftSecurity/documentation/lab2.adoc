== Lab 2: Implementing network isolation between running containers using Network Policies


=== Goal of Lab 2
The goal of this lab is to learn about how to implement network isolation between running containers in Red Hat OpenShift Container Platform using Software Defined Networking and Network Policies. First, we will create a few projects (K8s namespaces) and examine default out of the box network policies provided in OpenShift. Then, we will use Network Policies to restrict which applications/projects can talk to each other by restricting the network layer to provide that network isolation between running containers with Software Defined Networking and Network Policies.

=== Introduction

https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes Network Policies] are an easy way for Project Administrators to define exactly what ingress/egress traffic is allowed to any pod, from any other pod, including traffic from pods located in other projects. By default, all Pods in a project are accessible from all other Pods and network endpoints. To isolate one or more Pods in a project, you can create NetworkPolicy objects in that project to indicate the allowed incoming connections. Project administrators can create and delete NetworkPolicy objects within their own project.

If a Pod is matched by selectors in one or more NetworkPolicy objects, then it will accept only connections that are allowed by at least one of those NetworkPolicy objects. A Pod that is not selected by any NetworkPolicy objects is fully accessible.

=== Objective
The objective of the exercise below is to create multiple projects and to test application communication between them. You will introduce Network policies that will block application communications across projects and then allow some.

Project-c will have a hello-world application to which you will send requests.
Projects-a, b and c will all have a client application. The client from project-c will always be able to communication with hello-world in project-c since they are in the same project. However, depending on the network policies in place at times you will be blocked from communicating from clients in projects a and b to the hello-world application in project c.

=== Lab 2.1 Creating Projects and Labeling Namespaces

. As a cluster admin user, create 3 projects and label those namespaces.
+
[source]
----
oc new-project proj-a --display-name="Project A"
oc new-project proj-b --display-name="Project B"
oc new-project proj-c --display-name="Project C"

----

. In order to create Network Policies and allow applications in one namespace to be accesssed by applications running in only certain other namespace, you have to label the namespaces so they can be identified in Network Policies (and this is why you have to be 'cluster-admin' level user):
+
[source]
----
oc label namespace proj-a name=proj-a
oc label namespace proj-b name=proj-b
oc label namespace proj-c name=proj-c
----

. Now, let's look at the projects and labels we just created:
+
[source]
----
oc get projects proj-a proj-b proj-c --show-labels 
----
+
image:images/lab2.1-showlabels.png[]

=== Lab 2.1 Creating the 'hello world' microservice and client pod in proj-c

. Let's go into the project named *proj-c* and create 2 pods and a service.
+
[source]
----

[localhost ~]$ oc project proj-c
[localhost ~]$ oc new-app quay.io/bkozdemb/hello --name="hello"

----
This will create a new app, which is a 'hello world' container based on image stored in quay.io that’s built on the RHEL base python image. It runs a simple web server that prints 'hello'.

. Next, let's confirm that the 2 pods are starting to run.
+
[source]
----
oc get pods
----

+
image:images/lab2.1-ocgetpods.png[]

. Now, let's create and run the client pod, which is a Fedora base image. When the image is running, notice the command that’s being run (tail -f /dev/null), which essentially prevents the pod from running and then immediately quitting. This client pod will be used to run a curl command later.
+
[source]
----

[localhost ~]$ oc run client --image=fedora --command -- tail -f /dev/null
pod/client created
----

. Let's confirm that our client pod and hello world pod are now running.
+
[source]
----
oc get pods
----

+
image:images/lab2.1-ocgetpods2.png[]

. Next, let's create similar client pods in other projects named *proj-a* and *proj-b*.
+
[source]
----

[localhost ~]$ oc project proj-b
[localhost ~]$ oc run client  --image=fedora --command -- tail -f /dev/null
[localhost ~]$ oc project proj-a
[localhost ~]$ oc run client  --image=fedora --command -- tail -f /dev/null

----

. Notice that the projects , *proj-a* and *proj-b* just have client pods running.
+
[source]
----
oc get pods -n proj-a
oc get pods -n proj-b
----
+
image:images/lab2.1-ocgetpods3.png[]

. As we saw in the previous steps, *proj-c*, has both a client pod and the service (as a part of 'hello world' app).
+
[source]
----
oc get pods -n proj-c
----
+
image:images/lab2.1-ocgetpods4.png[]

. When the client pod is ready, display pod information with their labels.
+
[source]
----
oc get pods --show-labels
----
+
image:images/lab2.1-showlabels2.png[]


. Notice that the label that we’re using for Client pod is *run=client* which is created automatically by our previous 'oc run client' command.

. Next, from a different project, *proj-a*, connect to the Client container and try to access the *hello.proj-c* service in project, *proj-c*. The default network policy allows a client pod in *proj-a* to access the microservice in *proj-c*.

+
[source]
----
oc project proj-a
POD=`oc get pods --selector=run=client --output=custom-columns=NAME:.metadata.name --no-headers`
----
This command above simply assigns the pod name to the *POD* variable since some pods have random names by default so this command allows you to give a specific name to the pod.
+
[source]
----
echo $POD
----
+
image:images/lab2.1-echopod.png[]

+
This returns *client*, which is the pod name.
Next, go into the pod and curl the service in project, *proj-c*. Notice that this is allowed since it's open access by default.

+
[source]
----
oc rsh ${POD}
#Inside the pod you have to execute:
curl -v hello.proj-c:8080
----
+
image:images/lab2.1-curloutput1.png[]

. What you have seen so far is how Network Policies work by default in OpenShift. 
Now let's take a look at the default Network Policies in the OpenShift web console. URL of web console can be found by running command:
+
[source]
----
[localhost ~]$ oc whoami --show-console
https://console-openshift-console.apps.cluster-tx8sn.tx8sn.sandbox1590.opentlc.com
----

. Log into the OpenShift web console, then go to Projects and find the project, *proj-c*. Navigate into *proj-c*, then select *Networking* -> *Network Policies*.

+
image:images/lab2.1.10-webconsole2.png[]
image:images/lab2.1.10-webconsole1.png[]



. Notice that (in earlier versions of OpenShift) those two Network Policies are created by default:


* *allow-from-all-namespaces*: This is why we can hit services in the project, *proj-c* from other projects (such as projects, *proj-a* and *proj-b*).
* *allow-from-ingress-namespace*: This allows ingress from the router (outside in through the router).

+
NOTE:  In the recent versions of OpenShift 4.x those default Network Policies are no longer present. As a result, if no Network Policies are defined, all traffic is allowed.

=== Lab 2.2 Creating Network Policies for network isolation
. In the OpenShift web console, choose project, *proj-c*, and go to *Networking* -> *Network Policies*.

. Next, delete the 2 default Network Policies (*allow-from-all-namespaces* and *allow-from-ingress-namespace*) if you see them. Remember that if no Network Policies are defined, all traffic is allowed.
+
image:images/lab2.2.2-deletenetworkpolicies.png[]

. Now, create a new Network Policy in project, *proj-c* that denies traffic from other namespaces. It should be
the first example shown on the right in the Sample Network policies. Notice there are a lot of Sample Network Policies. Apply the first example *Limit access to the current namespace*. Click Try it. This creates the yaml. Next, press *create*.
+
image:images/lab2.2-createnetworkpolicies1.png[]
image:images/lab2.2-createnetworkpolicies2-new.png[]


. Now, navigate into *Networking* -> *Network Policies*. and notice that the *deny-other-namespaces* network policy is defined.
+
image:images/lab2.2-denyothernamespaces.png[]

. Next, try to curl the hello world service in project, *proj-c* from the client in *proj-a*. Notice that the curl fails this time.
+
[source]
----
oc rsh ${POD}
#Inside the pod you have to execute:
curl -v hello.proj-c:8080
----
+
image:images/lab2.2-curlfail.png[]
+
Remember to exit the pod with the `exit` command.


. Same kind of failure you would get if you try to access application running in *proj-c* from *proj-b* because the *deny-other-namespaces* Network Policy blocks traffic from ALL namespaces

=== Lab 2.3 Creating Network Policies for selective network access

. Here you will create additional Network Policy that will allow access to pods running in *proj-c* project from those running in different projects, selected by their labels. In the previous lab you created a Network Policy that denies access to all pods in *proj-c* from other projects. 
. Now, similar to Lab 2.2, let's create a Network Policy that is based on the sample "ALLOW traffic from all Pods in a particular namespace" policy. In the 'podSelector.matchLabels' section specify 'deployment:hello' to select the 'hello' labeled pods and in the 'namespaceSelector.matchLabels' section specify 'name:proj-a' to indicate that you will allow traffic from apps deployed in that namespace (recall that we labeled it with 'name:proj-a' in Lab 2.1). Press *Create* to create Network Policy
+
image:images/lab2.3-allow-traffic-from-proj-a.png[]

. Now, navigate into *Networking* -> *Network Policies*. and notice that the *web-allow-production* Network Policy is there:
+
image:images/lab2.3-policies-list.png[]

. Next, again try to access the 'hello world' service in project *proj-c* from the Client running in *proj-a*. Notice that the curl succeeds this time because ingress traffic is explicitely allowed from *proj-a* to our 'hello world' pod by the *web-allow-production* Network Policy:
+
[source]
----
[localhost ~]$ oc rsh ${POD}
sh-5.0# curl -v hello.proj-c:8080
----
+
image:images/lab2.3-curl-from-proj-a-ok.png[]

. Next, try to curl the 'hello world' service in project *proj-c* from the Client running in *proj-b*. Notice that the curl fails because the first Network Policy still blocks it and the second one is not applicable to pods running in *proj-b*:
+
[source]
----
[localhost ~]$ oc rsh ${POD}
sh-5.0# curl -v hello.proj-c:8080
----
+
image:images/lab2.3-curl-from-proj-b-fails.png[]

=== Summary

You have learned how to created multiple OpenShift projects/namespaces and test application communication between them. You also learned how to create declarative Network Policies that block application communications across projects and then allow application communications between selected applications running in specific namespaces. Network Policies when used propely are very powerful way to implement cloud native applications network security.

<<top>>

link:README.adoc#table-of-contents[ Table of Contents ]
